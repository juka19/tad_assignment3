---
title: "draft_final_assignment"
author: "Andrew Wells"
date: "11/18/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r }
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(readr)
library(dplyr)
library(quanteda.textplots)
library(uwot)
library(topicmodels)
library(tidytext)
library(ggrepel)
library(vader)
library(tidyr)
```

```{r }
df <- read_csv("https://raw.githubusercontent.com/juka19/tad_assignment3/main/data/data_11_28.csv")
```
## Creating a corpus
```{r }
df_corp <- df
df_corp <- df_corp %>% rename(text = summary)
corp <- corpus(df_corp)
```

##Creatung a dfm from the corpus
```{r }
dfmat <- corp %>%
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%
  tokens_remove(patter = stopwords("en")) %>%
  tokens_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
  dfm()

#Do we also want to lemmatize and apply word stems??

# tokens_replace(pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma) %>%

# tokens_wordstem() %>%
```

##Most common words in the corpus
```{r }
dfmat %>% colSums() %>% sort(decreasing = TRUE) %>% head(20)
```
#Wordcloud congress 115

```{r }
dfmat_115 <- dfm(corpus_subset(corp, session == 115), remove = stopwords("english"), remove_punct = TRUE,
                remove_numbers = TRUE) %>%
  dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
    dfm_trim(min_termfreq = 1)

textplot_wordcloud(dfmat_115, max_words = 300)
```

#Wordcloud congress 116

```{r }
dfmat_116 <- dfm(corpus_subset(corp, session == 116), remove = stopwords("english"), remove_punct = TRUE, 
                remove_numbers = TRUE) %>%
  dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
    dfm_trim(min_termfreq = 1)

textplot_wordcloud(dfmat_116, max_words = 300)
```
#Wordcloud congress 117

```{r }
dfmat_117 <- dfm(corpus_subset(corp, session == 117), remove = stopwords("english"), remove_punct = TRUE,
                remove_numbers = TRUE) %>%
  dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
    dfm_trim(min_termfreq = 1)

textplot_wordcloud(dfmat_117, max_words = 300)
```


#Wordcloud comparing Democrats and Republicans
```{r }
#Need to get sponsor party data

dfmatRepDem <- dfm(corp, remove = stopwords("english"), remove_punct = TRUE, groups = corp$Party) %>%
    dfm_trim(min_termfreq = 3)

textplot_wordcloud(dfmatRepDem, comparison = TRUE, max_words = 300,
                   color = c("blue", "red"))

```


#Wordcloud comparing Congresses
```{r }
dfmatCon <- dfm(corp, remove = stopwords("english"), remove_punct = TRUE, groups = corp$Congress) %>%
    dfm_trim(min_termfreq = 3)

textplot_wordcloud(dfmatCon, comparison = TRUE, max_words = 300,
                   color = c("blue", "red"))

```

## Most common words (overall)
```{r}
#Overall most common words

tfreq <- dfmat %>%
  textstat_frequency() %>%
  head(20)

tfreq$feature <- factor(tfreq$feature, levels=tfreq$feature) 

ggplot(tfreq, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency",
        x ="Frequency", y = "Feature") +
  theme_minimal() 
```
## Most common words (by session)

```{r }
# Most common words by Congressional session 115
tfreq_115 <- dfmat_115 %>%
  textstat_frequency() %>%
  head(20)

tfreq_115$feature <- factor(tfreq_115$feature, levels=tfreq_115$feature) 

ggplot(tfreq_115, aes(x=frequency, y=feature)) +
  geom_col() + labs(title="Feature (word) frequency for 115 session (2017 - 2018)",
        x ="Frequency", y = "Feature") +
  theme_minimal()
```

```{r }
# Most common words by Congressional session 116
tfreq_116 <- dfmat_116 %>%
  textstat_frequency() %>%
  head(20)

tfreq_116$feature <- factor(tfreq_116$feature, levels=tfreq_116$feature) 

ggplot(tfreq_116, aes(x=frequency, y=feature)) +
  geom_col() + labs(title="Feature (word) frequency for 116 session (2019 - 2020)",
        x ="Frequency", y = "Feature") +
  theme_minimal()
```

```{r}
# Most common words by Congressional session 117
tfreq_117 <- dfmat_117 %>%
  textstat_frequency() %>%
  head(20)

tfreq_117$feature <- factor(tfreq_117$feature, levels=tfreq_117$feature) 

ggplot(tfreq_117, aes(x=frequency, y=feature)) +
  geom_col() + labs(title="Feature (word) frequency for 117 session (2021 - 2022)",
        x ="Frequency", y = "Feature") +
  theme_minimal()
```

##Comparing subgroups
```{r}
df <- df %>%
  mutate(session_written = ifelse(session == 115, "session_115", 
                             ifelse(session == 116, "session_116",
                             ifelse(session == 117, "session_117", NA))))

ytfreq <- dfmat %>% 
  textstat_frequency(groups = df$session_written) %>%
  pivot_wider(id_cols=feature, names_from=group, values_from=frequency)
```

```{r}
#Comparing sessions 115 and 116

#Plot #1: differences in frequency by session
ggplot(ytfreq, aes(x= session_116, y= session_115)) + 
  geom_point() + theme_minimal()

#Plot #2: coloring changes by session
ytfreq$change_115_116 <- log(ytfreq$session_116 / ytfreq$session_115)
max_change <- max(abs(ytfreq$change_115_116), na.rm=TRUE)

ggplot(ytfreq, aes(x=session_116, y=session_115, fill=change_115_116)) + 
  geom_point(color="grey", shape=21) +
  scale_fill_gradientn(
    colors = c("#4575b4","white","#d73027"),
    values = scales::rescale(c(max_change*-1,0,max_change)), limits = c(max_change*-1,max_change))+
theme_bw()

#Plot 3: labeling words with biggest change
labels <- ytfreq %>% 
  rowwise() %>% 
  mutate(max_value = max(session_116, session_115)) %>% 
  filter((abs(change_115_116)>0.4 & max_value>2.5) )

ggplot(ytfreq, aes(x=session_116, y=session_115, fill=change_115_116)) + 
  geom_point(color="grey", shape=21) +
  scale_fill_gradientn(
    colors = c("#4575b4","white","#d73027"),
    values = scales::rescale(c(max_change*-1,0,max_change)), limits = c(max_change*-1,max_change)) + theme_bw() +
  geom_label_repel(data=labels, aes(label=feature), min.segment.length = 0)
```

```{r}
#Comparing sessions 116 and 117
ggplot(ytfreq, aes(x= session_117, y= session_116)) + 
  geom_point() + theme_minimal()


#Plot #2: coloring changes by session
ytfreq$change_116_117 <- log(ytfreq$session_117 / ytfreq$session_116)
max_change <- max(abs(ytfreq$change_116_117), na.rm=TRUE)

ggplot(ytfreq, aes(x=session_117, y=session_116, fill=change_116_117)) + 
  geom_point(color="grey", shape=21) +
  scale_fill_gradientn(
    colors = c("#4575b4","white","#d73027"),
    values = scales::rescale(c(max_change*-1,0,max_change)), limits = c(max_change*-1,max_change))+
theme_bw()

#Plot 3: labeling words with biggest change
labels <- ytfreq %>% 
  rowwise() %>% 
  mutate(max_value = max(session_117, session_116)) %>% 
  filter((abs(change_116_117)>0.4 & max_value>2.5) )

ggplot(ytfreq, aes(x=session_117, y=session_116, fill=change_116_117)) + 
  geom_point(color="grey", shape=21) +
  scale_fill_gradientn(
    colors = c("#4575b4","white","#d73027"),
    values = scales::rescale(c(max_change*-1,0,max_change)), limits = c(max_change*-1,max_change)) + theme_bw() +
  geom_label_repel(data=labels, aes(label=feature), min.segment.length = 0)
```

```{r}
sims <- textstat_simil(dfm_corp, method="cosine")

sim_df <- as.data.frame(sims, upper = TRUE)

sim_df
```

```{r }
df <- df %>%
  mutate(partisan = ifelse(cosponsor_D_perc > 0.5,"Dem", 
                       ifelse(cosponsor_D_perc < 0.5,"Rep",
                              ifelse(cosponsor_D_perc == 0.5,"Neutral", NA))))

corp <- corpus(df$summary)

dfmat2 <- corp %>%
  tokens(remove_punct = TRUE) %>%
  tokens_remove(patter = stopwords("en")) %>%
  dfm() %>%
  dfm_trim(min_termfreq = 5)

embeddings <- umap(as.matrix(dfmat2)) 

df$x <- embeddings[,1]
df$y <- embeddings[,2]

colordict <- c( "Dem"="blue","Rep"="red", "Neutral"="yellow")

p <- ggplot(df, aes(x, y, fill=partisan)) + 
  geom_point(color="grey", shape=21, size=0.5) + 
  scale_fill_manual(values=colordict) +
  theme_bw() +
  coord_fixed() 

p
```

##Topic modeling

```{r }
#Topics for session 115 (2017 - 2018)

lda_115 <- LDA(dfmat_115, 5)

topic_words_115 <- tidy(lda_115, matrix="beta") %>% 
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta) 

topic_words_115

topic_words_115 %>%
  mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot(aes(beta, term, fill = factor(topic))) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") + scale_y_reordered() + theme_minimal()
```

```{r }
#Topics for session 116 (2019 - 2020)

lda_116 <- LDA(dfmat_116, 5)

topic_words_116 <- tidy(lda_116, matrix="beta") %>% 
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta) 

topic_words_116

topic_words_116 %>%
  mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot(aes(beta, term, fill = factor(topic))) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") + scale_y_reordered() + theme_minimal()
```

```{r }
#Topics for session 117 (2021 - 2022)
raw.sum = apply(dfmat_117,1,FUN=sum) #Removing non-0 rows from dfmat
dfmat_117 = dfmat_117[raw.sum != 0, ]

lda_117 <- LDA(dfmat_117, 5)

topic_words_117 <- tidy(lda_117, matrix="beta") %>% 
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta) 

topic_words_117

topic_words_117 %>%
  mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot(aes(beta, term, fill = factor(topic))) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") + scale_y_reordered() + theme_minimal()
```

##Sentiments
```{r}
lex <- get_sentiments("afinn")

text_tokens <- tidy(dfmat) %>% 
  inner_join(lex, by=c("term" = "word"))

doc_sentiments <- tidy(dfmat) %>% 
  inner_join(lex, by=c("term" = "word")) %>% 
  mutate(value=value*count) %>% 
  group_by(document) %>%
  summarise(value = sum(value))

doc_sentiments
```

```{r}
#Loading df with VADER compount scores

summary_sentiment <- read_csv("https://raw.githubusercontent.com/juka19/tad_assignment3/main/data/data_w_vader.csv")
```


```{r}
library(tidyr)
#tweet_sentiment$date <- as.Date(tweet_sentiment$created) 
summary_sentiment <- summary_sentiment %>%
  mutate(partisan = ifelse(cosponsor_D_perc > 0.5,"Dem", 
                       ifelse(cosponsor_D_perc < 0.5,"Rep",
                              ifelse(cosponsor_D_perc == 0.5,"Neutral", NA))))

wide_sentiment <- summary_sentiment %>%
  group_by(partisan, date) %>%
  summarise(score = mean(compound)) %>% 
  pivot_wider(names_from = partisan, values_from = score) %>%
  select(-c("Neutral", "NA"))


days <- data.frame(date = seq(as.Date("2017-01-01"),as.Date("2022-12-31"),1))

daily_sentiment <- days %>% 
  left_join(wide_sentiment) %>% 
  pivot_longer(cols = -date, names_to="partisan", values_to="score") %>%
  group_by(partisan) %>% 
  arrange(date) %>%
  mutate(score7 = data.table::frollmean(score, 7))

daily_sentiment %>% head()

ggplot(daily_sentiment, aes(date, colour=partisan)) +
  geom_point(aes(y=score), size=0.5) + 
  geom_line(aes(y=score7)) + theme_minimal()
```


