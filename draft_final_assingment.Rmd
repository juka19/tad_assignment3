---
title: "draft_final_assignment"
author: "Andrew Wells"
date: "11/18/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r }
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(readr)
library(dplyr)
library(quanteda.textplots)
library(uwot)
library(topicmodels)
library(tidytext)
library(ggrepel)
library(vader)
library(tidyr)
library(tidymodels) 
library(textrecipes)
```

```{r }
df <- read_csv("https://raw.githubusercontent.com/juka19/tad_assignment3/main/data/data_11_28.csv")
```

## Creating party variable
```{r }
#if two thirds of the sponsors are democrats, we consider the bill democrat-dominated
#same for republicans
#if there is no clear majority, they are "Both"
df$party <- ifelse(df$cosponsor_D_perc > 0.66, "Democrat", ifelse(df$cosponsor_R_perc > 0.66, "Republican", "Both"))
```

```{r}
ggplot(df, aes(x = cosponsor_D_perc)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white") +
  geom_density(alpha=.1, fill="blue") +
  labs(title="Density of bill cosposor party",
       x ="Cosponsor party composition", y = "Density", 
       caption = "Numbers represent proportion of cosponsors from Democratic party, 
       so 0.0 represents bills that were fully Republican and 1.0 represents 
       bills that were fully Democrat.") +
  theme_minimal()
```
  

## Creating a corpus
```{r }
df_corp <- df
df_corp <- df_corp %>% rename(text = summary)
corp <- corpus(df_corp)
```

##Creatung a dfm from the corpus
```{r }
dfmat <- corp %>%
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%
  tokens_remove(patter = stopwords("en")) %>%
  tokens_replace(pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma) %>%
  tokens_wordstem() %>%
  tokens_remove(c("sec","bill","act", "section", "funds", "shall","must", "use", "author")) %>%
  dfm()

#Do we also want to lemmatize and apply word stems??
# tokens_replace(pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma) %>%
# tokens_wordstem() %>%
```

##Most common words in the corpus
```{r }
dfmat %>% colSums() %>% sort(decreasing = TRUE) %>% head(20)
```
#Wordcloud congress 115

```{r }
dfmat_115 <- dfm_subset(dfmat, session == 115)

textplot_wordcloud(dfmat_115, max_words = 300)
```

#Wordcloud congress 116

```{r }
dfmat_116 <- dfm_subset(dfmat, session == 116)

textplot_wordcloud(dfmat_116, max_words = 300)
```
#Wordcloud congress 117

```{r }
dfmat_117 <- dfm_subset(dfmat, session == 117)

textplot_wordcloud(dfmat_117, max_words = 300)
```


#Wordcloud comparing Democrats and Republicans
```{r }
dfmatRepDem <- dfm(corp, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups =
                     corp$party) %>%
  dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
  dfm_trim(min_termfreq = 3)

textplot_wordcloud(dfmatRepDem, comparison = TRUE, max_words = 300,
                   color = c("green","blue", "red"))
```

#Wordcloud congress parties 115

```{r }

corp_115 <- df %>% filter(session == 115) %>% rename(text = summary) %>% corpus()
modelpart15 <- dfm(corp_115, remove = stopwords("english"),remove_numbers = TRUE, remove_punct = TRUE, groups = corp_115$party) %>%
   dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
  dfm_trim(min_termfreq = 3)

mp15 <- textplot_wordcloud(modelpart15, comparison = TRUE, max_words = 300,
                   color = c("green","blue", "red"))
mp15
```



#Wordcloud congress parties 116

```{r }

corp_116 <- df %>% filter(session == 116) %>% rename(text = summary) %>% corpus()
modelpart16 <- dfm(corp_116, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups = corp_116$party) %>% dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
  dfm_trim(min_termfreq = 3)

mp16 <- textplot_wordcloud(modelpart16, comparison = TRUE, max_words = 300,
                   color = c("green","blue", "red"))
mp16

```


#Wordcloud congress parties 117

```{r }

corp_117 <- df %>% filter(session == 117) %>% rename(text = summary) %>% corpus()
modelpart17 <- dfm(corp_117, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups = corp_117$party) %>% dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
  dfm_trim(min_termfreq = 3)

mp17 <- textplot_wordcloud(modelpart17, comparison = TRUE, max_words = 300,
                   color = c("green","blue", "red"))

mp17

```

#Wordcloud comparing Congresses
```{r }
dfmatCon <- dfm(corp, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups = corp$session) %>%dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
  dfm_trim(min_termfreq = 3)

textplot_wordcloud(dfmatCon, comparison = TRUE, max_words = 300,
                   color = c("blue", "red"))
```

## Most common words (overall)
```{r}
#Overall most common words

tfreq <- dfmat %>%
  textstat_frequency() %>%
  head(20)

tfreq$feature <- factor(tfreq$feature, levels=tfreq$feature) 

ggplot(tfreq, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency",
        x ="Frequency", y = "Feature") +
  theme_minimal() 
```
## Most common words by party 

```{r}
#Republicans
dfmat_r <- dfm_subset(dfmat, party == "Republican")

tfreq_r <- dfmat_r %>%
  textstat_frequency() %>%
  head(20)

tfreq_r$feature <- factor(tfreq_r$feature, levels=tfreq_r$feature) 

ggplot(tfreq_r, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency for Republican Bills",
        x ="Frequency", y = "Feature") +
  theme_minimal() 
```

```{r}
#Democrats
dfmat_d <- dfm_subset(dfmat, party == "Democrat")

tfreq_d <- dfmat_d %>%
  textstat_frequency() %>%
  head(20)

tfreq_d$feature <- factor(tfreq_d$feature, levels=tfreq_d$feature) 

ggplot(tfreq_d, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency for Democrat Bills",
        x ="Frequency", y = "Feature") +
  theme_minimal() 
```

```{r}
party_corp <- corpus_subset(corp, 
                            party %in% c("Democrat", "Republican"))

# Create a dfm grouped by party
party_dfm <- tokens(party_corp, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%
  tokens_remove(patter = stopwords("en")) %>%
  tokens_replace(pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma) %>%
  tokens_wordstem() %>%
  tokens_remove(c("sec","bill","act", "section", "funds", "shall","must", "use", "author", "may")) %>%
  tokens_group(groups = party) %>%
  dfm()

# Calculate keyness and determine Republican as target group
result_keyness <- textstat_keyness(party_dfm, target = "Republican")

# Plot estimated word keyness
textplot_keyness(result_keyness) 
```


## Most common words (by session)

```{r }
# Most common words by Congressional session 115
tfreq_115 <- dfmat_115 %>%
  textstat_frequency() %>%
  head(20)

tfreq_115$feature <- factor(tfreq_115$feature, levels=tfreq_115$feature) 

ggplot(tfreq_115, aes(x=frequency, y=feature)) +
  geom_col() + labs(title="Feature (word) frequency for 115 session (2017 - 2018)",
        x ="Frequency", y = "Feature") +
  theme_minimal()
```

```{r }
# Most common words by Congressional session 116
tfreq_116 <- dfmat_116 %>%
  textstat_frequency() %>%
  head(20)

tfreq_116$feature <- factor(tfreq_116$feature, levels=tfreq_116$feature) 

ggplot(tfreq_116, aes(x=frequency, y=feature)) +
  geom_col() + labs(title="Feature (word) frequency for 116 session (2019 - 2020)",
        x ="Frequency", y = "Feature") +
  theme_minimal()
```

```{r}
# Most common words by Congressional session 117
tfreq_117 <- dfmat_117 %>%
  textstat_frequency() %>%
  head(20)

tfreq_117$feature <- factor(tfreq_117$feature, levels=tfreq_117$feature) 

ggplot(tfreq_117, aes(x=frequency, y=feature)) +
  geom_col() + labs(title="Feature (word) frequency for 117 session (2021 - 2022)",
        x ="Frequency", y = "Feature") +
  theme_minimal()
```

##Comparing subgroups
```{r}
df <- df %>%
  mutate(session_written = ifelse(session == 115, "session_115", 
                             ifelse(session == 116, "session_116",
                             ifelse(session == 117, "session_117", NA))))

ytfreq <- dfmat %>% 
  textstat_frequency(groups = df$session_written) %>%
  pivot_wider(id_cols=feature, names_from=group, values_from=frequency)
```

```{r}
#Comparing sessions 115 and 116

#Plot #1: differences in frequency by session
ggplot(ytfreq, aes(x= session_116, y= session_115)) + 
  geom_point() + theme_minimal()

#Plot #2: coloring changes by session
ytfreq$change_115_116 <- log(ytfreq$session_116 / ytfreq$session_115)
max_change <- max(abs(ytfreq$change_115_116), na.rm=TRUE)

ggplot(ytfreq, aes(x=session_116, y=session_115, fill=change_115_116)) + 
  geom_point(color="grey", shape=21) +
  scale_fill_gradientn(
    colors = c("#4575b4","white","#d73027"),
    values = scales::rescale(c(max_change*-1,0,max_change)), limits = c(max_change*-1,max_change))+
theme_bw()

#Plot 3: labeling words with biggest change
labels <- ytfreq %>% 
  rowwise() %>% 
  mutate(max_value = max(session_116, session_115)) %>% 
  filter((abs(change_115_116)>0.4 & max_value>2.5) )

ggplot(ytfreq, aes(x=session_116, y=session_115, fill=change_115_116)) + 
  geom_point(color="grey", shape=21) +
  scale_fill_gradientn(
    colors = c("#4575b4","white","#d73027"),
    values = scales::rescale(c(max_change*-1,0,max_change)), limits = c(max_change*-1,max_change)) + theme_bw() +
  geom_label_repel(data=labels, aes(label=feature), min.segment.length = 0)
```

```{r}
#Comparing sessions 116 and 117
ggplot(ytfreq, aes(x= session_117, y= session_116)) + 
  geom_point() + theme_minimal()


#Plot #2: coloring changes by session
ytfreq$change_116_117 <- log(ytfreq$session_117 / ytfreq$session_116)
max_change <- max(abs(ytfreq$change_116_117), na.rm=TRUE)

ggplot(ytfreq, aes(x=session_117, y=session_116, fill=change_116_117)) + 
  geom_point(color="grey", shape=21) +
  scale_fill_gradientn(
    colors = c("#4575b4","white","#d73027"),
    values = scales::rescale(c(max_change*-1,0,max_change)), limits = c(max_change*-1,max_change))+
theme_bw()

#Plot 3: labeling words with biggest change
labels <- ytfreq %>% 
  rowwise() %>% 
  mutate(max_value = max(session_117, session_116)) %>% 
  filter((abs(change_116_117)>0.4 & max_value>2.5) )

ggplot(ytfreq, aes(x=session_117, y=session_116, fill=change_116_117)) + 
  geom_point(color="grey", shape=21) +
  scale_fill_gradientn(
    colors = c("#4575b4","white","#d73027"),
    values = scales::rescale(c(max_change*-1,0,max_change)), limits = c(max_change*-1,max_change)) + theme_bw() +
  geom_label_repel(data=labels, aes(label=feature), min.segment.length = 0)
```

```{r}
sims <- textstat_simil(dfmat, method="cosine")

sim_df <- as.data.frame(sims, upper = TRUE)

sim_df
```

```{r }
corp2 <- corpus(df$summary)

dfmat2 <- corp2 %>%
  tokens(remove_punct = TRUE) %>%
  tokens_remove(patter = stopwords("en")) %>%
  dfm() %>%
  dfm_trim(min_termfreq = 5)

embeddings <- umap(as.matrix(dfmat2)) 

df$x <- embeddings[,1]
df$y <- embeddings[,2]

colordict <- c( "Democrat"="blue","Republican"="red", "Both"="yellow")

ggplot(df, aes(x, y, fill=party)) + 
  geom_point(color="grey", shape=21, size=0.5) + 
  scale_fill_manual(values=colordict) +
  theme_bw()
```

##Topic modeling

```{r }
#Topics for session 115 (2017 - 2018)

lda_115 <- LDA(dfmat_115, 5)

topic_words_115 <- tidy(lda_115, matrix="beta") %>% 
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta) 

topic_words_115

topic_words_115 %>%
  mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot(aes(beta, term, fill = factor(topic))) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") + scale_y_reordered() + theme_minimal()
```

```{r }
#Topics for session 116 (2019 - 2020)

lda_116 <- LDA(dfmat_116, 5)

topic_words_116 <- tidy(lda_116, matrix="beta") %>% 
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta) 

topic_words_116

topic_words_116 %>%
  mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot(aes(beta, term, fill = factor(topic))) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") + scale_y_reordered() + theme_minimal()
```

```{r }
#Topics for session 117 (2021 - 2022)
raw.sum = apply(dfmat_117,1,FUN=sum) #Removing non-0 rows from dfmat
dfmat_117 = dfmat_117[raw.sum != 0, ]

lda_117 <- LDA(dfmat_117, 5)

topic_words_117 <- tidy(lda_117, matrix="beta") %>% 
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta) 

topic_words_117

topic_words_117 %>%
  mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot(aes(beta, term, fill = factor(topic))) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") + scale_y_reordered() + theme_minimal()
```

##Sentiments

```{r}
#Loading df with VADER compount scores

summary_sentiment <- read_csv("https://raw.githubusercontent.com/juka19/tad_assignment3/main/data/data_w_vader.csv")
```
```{r}
ggplot(summary_sentiment, aes(x = compound)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white") +
  geom_density(alpha=.1, fill="blue") +
  labs(title="Density of compount sentiment scores",
       x ="Sentiment scores", y = "Density") +
  theme_minimal()
```

```{r}
summary_sentiment$party <- ifelse(summary_sentiment$cosponsor_D_perc > 0.66, "Democrat", ifelse(summary_sentiment$cosponsor_R_perc > 0.66, "Republican", "Both"))
  
wide_sentiment <- summary_sentiment %>%
  group_by(party, date) %>%
  summarise(score = mean(compound)) %>% 
  pivot_wider(names_from = party, values_from = score) %>%
  select(-c("Both", "NA"))

days <- data.frame(date = seq(as.Date("2017-01-01"),as.Date("2022-12-31"),1))

daily_sentiment <- days %>% 
  left_join(wide_sentiment) %>% 
  pivot_longer(cols = -date, names_to="party", values_to="score") %>%
  group_by(party) %>% 
  arrange(date) %>%
  mutate(score7 = data.table::frollmean(score, 7))

ggplot(daily_sentiment, aes(x=date, y = score, colour=party)) +
  geom_point(aes(y=score), size=1) + 
  geom_line(aes(y=score7)) + 
  theme_minimal() + 
  geom_smooth(method = "lm")
```
```{r}
summary_sentiment$party_full <- ifelse(summary_sentiment$cosponsor_D_perc == 1.0, "Dem", ifelse(summary_sentiment$cosponsor_R_perc == 1.0, "Rep", NA))

wide_sentiment_full <- summary_sentiment %>%
  drop_na(party_full) %>%
  group_by(party_full, date) %>%
  summarise(score = mean(compound)) %>% 
  pivot_wider(names_from = party_full, values_from = score)

days_115 <- data.frame(date = seq(as.Date("2017-01-01"),as.Date("2018-12-31"),1))

daily_sentiment_115 <- days_115 %>% 
  left_join(wide_sentiment_full) %>% 
  pivot_longer(cols = -date, names_to="party_full", values_to="score") %>%
  group_by(party_full) %>% 
  arrange(date) %>%
  mutate(score7 = data.table::frollmean(score, 7))

daily_sentiment %>% head()

ggplot(daily_sentiment_115, aes(x=date, y = score, colour=party_full)) +
  geom_point(aes(y=score), size=1) + 
  geom_line(aes(y=score7)) + 
  theme_minimal() + 
  geom_smooth(method = "lm")
```

```{r}
days_116 <- data.frame(date = seq(as.Date("2019-01-01"),as.Date("2020-12-31"),1))

daily_sentiment_116 <- days_116 %>% 
  left_join(wide_sentiment_full) %>% 
  pivot_longer(cols = -date, names_to="party_full", values_to="score") %>%
  group_by(party_full) %>% 
  arrange(date) %>%
  mutate(score7 = data.table::frollmean(score, 7))

daily_sentiment_116 %>% head()

ggplot(daily_sentiment_116, aes(x=date, y = score, colour=party_full)) +
  geom_point(aes(y=score), size=1) + 
  geom_line(aes(y=score7)) + 
  theme_minimal() + 
  geom_smooth(method = "lm")
```
```{r}
days_117 <- data.frame(date = seq(as.Date("2021-01-01"),as.Date("2022-12-31"),1))

daily_sentiment_117 <- days_117 %>% 
  left_join(wide_sentiment_full) %>% 
  pivot_longer(cols = -date, names_to="party_full", values_to="score") %>%
  group_by(party_full) %>% 
  arrange(date) %>%
  mutate(score7 = data.table::frollmean(score, 7))

ggplot(daily_sentiment_117, aes(x=date, y = score, colour=party_full)) +
  geom_point(aes(y=score), size=1) + 
  geom_line(aes(y=score7)) + 
  theme_minimal() + 
  geom_smooth(method = "lm")
```
##Some machine learning

```{r}
#Prediction model to see if can predict if bill is Republican or Democrat

#Keep only bills that are over 66% Democrat or Republican
df1 <- df %>%
  filter(party != "Both")

df1 <- df1[sample(nrow(df1)),]
df1$env <- 0
df1$env[df1$party == "Republican"] <- 1
df1$env <- factor(df1$env)

df_split <- initial_split(df1, prop=0.8) 
train_data <- training(df_split) 
test_data <- testing(df_split)

rec <- recipe(env ~ summary, data = train_data) %>% 
  step_tokenize(summary) %>% 
  step_tokenfilter(summary, max_tokens = 1e3) %>% 
  step_tfidf(summary)

model <- svm_linear(mode="classification")

wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(model)

model_fit <- wf %>% 
  fit(train_data)
```

```{r}
test_data$prediction <- predict(model_fit, test_data)$.pred_class

scorer <- metric_set( yardstick::accuracy, 
                      yardstick::precision, 
                      yardstick::recall, 
                      yardstick::f_meas
)

scorer(test_data, truth=env, estimate=prediction, event_level="second")
```

```{r}
#Prediction model to see if can predict if bill is focusing on health (as policy area)

df2 <- df[sample(nrow(df)),]
df2$env <- 0
df2$env[df2$policy_area == "Health"] <- 1
df2$env <- factor(df2$env)

df_split <- initial_split(df2, prop=0.8) 
train_data <- training(df_split) 
test_data <- testing(df_split)

rec <- recipe(env ~ summary, data = train_data) %>% 
  step_tokenize(summary) %>% 
  step_tokenfilter(summary, max_tokens = 1e3) %>% 
  step_tfidf(summary)

model <- svm_linear(mode="classification")

wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(model)

model_fit <- wf %>% 
  fit(train_data)
```

```{r}
test_data$prediction <- predict(model_fit, test_data)$.pred_class

scorer <- metric_set( yardstick::accuracy, 
                      yardstick::precision, 
                      yardstick::recall, 
                      yardstick::f_meas
)

scorer(test_data, truth=env, estimate=prediction, event_level="second")
```

