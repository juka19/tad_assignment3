---
title: "junk_final_assignment"
author: "Andrew Wells"
date: "11/18/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(readr)
```

```{r}
url <- "https://raw.githubusercontent.com/juka19/tad_assignment3/main/data/dummy_df.csv"

df <- read_csv(url)
```

## Cleaning data
```{r}
#Trying to extract party name (D or R) from the sponsor, will figure this out with Regex

df %>%
  mutate(party = ifelse(str_detect(Sponsor, "[R-"), "Republican", 
                   ifelse(str_detect(Sponsor, "[D-"), "Democrat", NA)))
```

## Creating document feature matrix
```{r}
dfmat <- df$Summary %>%
  tokens(remove_punct = TRUE) %>%
  tokens_remove(patter = stopwords("en")) %>%
  dfm()
```

## Most common words
```{r}
#Overall most common words

tfreq <- dfmat %>%
  textstat_frequency() %>%
  head(20)

tfreq$feature <- factor(tfreq$feature, levels=tfreq$feature) 

ggplot(tfreq, aes(x=frequency, y=feature)) +
geom_col() + 
  theme_minimal()
```

```{r}
# Most common words by Congressional sesson
df_116 <- df %>%
  filter(Congress == "116")

df_117 <- df %>%
  filter(Congress == "117")

dfmat_116 <- df_116$Summary %>%
  tokens(remove_punct = TRUE) %>%
  tokens_remove(patter = stopwords("en")) %>%
  dfm()

dfmat_117 <- df_117$Summary %>%
  tokens(remove_punct = TRUE) %>%
  tokens_remove(patter = stopwords("en")) %>%
  dfm()

tfreq_116 <- dfmat_116 %>%
  textstat_frequency() %>%
  head(20)

tfreq_117 <- dfmat_117 %>%
  textstat_frequency() %>%
  head(20)

tfreq_116$feature <- factor(tfreq_116$feature, levels=tfreq_116$feature) 
tfreq_117$feature <- factor(tfreq_117$feature, levels=tfreq_117$feature) 

p116 <- ggplot(tfreq_116, aes(x=frequency, y=feature)) +
  geom_col() + 
  theme_minimal()

p117 <- ggplot(tfreq_117, aes(x=frequency, y=feature)) +
  geom_col() + 
  theme_minimal()

p116
```

```{r}
p117
```


```{r}
df$era <- ifelse(df$Congress == 116, "Pre", "Post")

ytfreq <- dfmat %>% 
  textstat_frequency(groups = df$era) %>%
  pivot_wider(id_cols=feature, names_from=group, values_from=frequency)
              
ggplot(ytfreq, aes(x="Post", y="Pre")) + 
  geom_point() +
  coord_fixed()
```
```{r}
sims <- textstat_simil(dfmat, method="cosine")

sim_df <- as.data.frame(sims, upper = TRUE)

sim_df
```
```{r}
library(uwot)

corp <- corpus(df$Summary)

dfmat2 <- corp %>%
  tokens(remove_punct = TRUE) %>%
  tokens_remove(patter = stopwords("en")) %>%
  dfm() %>%
  dfm_trim(min_termfreq = 5)

embeddings <- umap(as.matrix(dfmat2)) 

df$x <- embeddings[,1]
df$y <- embeddings[,2]

colordict <- c( "Labour"="red","LibDems"="yellow", "Conservatives"="Blue","Greens"="green")

p <- ggplot(df, aes(x, y, fill=party)) + 
  geom_point(color="grey", shape=21, size=0.5) + 
  scale_fill_manual(values=colordict) + theme_bw() +
  coord_fixed() 

p
```

```{r}
library(topicmodels) 
lda <- LDA(dfmat, 3)

lda
```
```{r}
library(tidytext)

topic_words <- tidy(lda, matrix="beta") %>% 
  group_by(topic) %>%
  slice_max(beta, n = 3) %>%
  ungroup() %>%
  arrange(topic, -beta) 

topic_words

topic_words %>%
  mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot(aes(beta, term, fill = factor(topic))) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") + scale_y_reordered()
```

##Sentiments
```{r}
lex <- get_sentiments("afinn")

text_tokens <- tidy(dfmat) %>% 
  inner_join(lex, by=c("term" = "word"))

doc_sentiments <- tidy(dfmat) %>% 
  inner_join(lex, by=c("term" = "word")) %>% 
  mutate(value=value*count) %>% 
  group_by(document) %>%
  summarise(value = sum(value))

doc_sentiments
```

```{r}
library(vader)

sentiments <- vader_df(df$Summary)

summary_sentiment <- cbind(df, select(sentiments,-text))

pos <- summary_sentiment %>% 
  arrange(desc(compound)) %>% 
  head()

for( i in rownames(pos) ) { 
  print(pos[i, "text"]) 
  print(pos[i, "compound"])
}

neg <- summary_sentiment %>% 
  arrange(compound) %>% 
  head()

for( i in rownames(neg) ) { 
  print(neg[i, "text"]) 
  print(neg[i, "compound"])
}
```




