toJSON() %>%
fromJSON()) -> test
add_info$metadata %>%
map_dfr(function(x) x %>%
toJSON() %>%
fromJSON() %>%
as.data.frame()) -> test
test[1] %>% fromJSON()
test$.[1] %>% fromJSON()
json <- fromJSON(file = "GitHub\\tad_assignment3\\no.txt")
gc()
library(bupaR)
library(eventdataR)
library(processanimateR)
library(tidyverse)
library(rjson)
json <- fromJSON(file = "GitHub\\tad_assignment3\\no.txt")
test <- json %>% as.tibble()
as.data.frame(json) -> test
json <- fromJSON(file = "GitHub\\tad_assignment3\\no.txt", simplify = T)
json %>% names(+)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
install.packages('tidyverse')
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
install.packages('tidyverse')
library(tidyverse)
.libPaths()
install.packages('rlang')
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidyverse, lib.loc = .libPaths()[1])
library(tidyverse, lib.loc = .libPaths()[2])
remove.packages('rlang')
remove.packages('rlang', lib = .libPaths()[1])
remove.packages('rlang', lib = .libPaths()[2])
remove.packages('rlang', lib = .libPaths()[3])
.libPaths()
.libPaths()[2]
.libPaths()[1]
install.packages('rlang')
install.packages("rlang")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(readr)
library(dplyr)
library(quanteda.textplots)
library(uwot)
library(topicmodels)
library(tidytext)
library(ggrepel)
library(vader)
library(tidyr)
library(tidymodels)
library(textrecipes)
df <- read_csv("https://raw.githubusercontent.com/juka19/tad_assignment3/main/data/data_11_28.csv")
#if two thirds of the sponsors are democrats, we consider the bill democrat-dominated
#same for republicans
#if there is no clear majority, they are "Both"
df$party <- ifelse(df$cosponsor_D_perc > 0.66, "Democrat", ifelse(df$cosponsor_R_perc > 0.66, "Republican", "Both"))
ggplot(df, aes(x = cosponsor_D_perc)) +
geom_histogram(aes(y=..density..), colour="black", fill="white") +
geom_density(alpha=.1, fill="blue") +
labs(title="Density of bill cosposor party",
x ="Cosponsor party composition", y = "Density",
caption = "Numbers represent proportion of cosponsors from Democratic party,
so 0.0 represents bills that were fully Republican and 1.0 represents
bills that were fully Democrat.") +
theme_minimal()
df_corp <- df
df_corp <- df_corp %>% rename(text = summary)
corp <- corpus(df_corp)
dfmat <- corp %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%
tokens_remove(patter = stopwords("en")) %>%
tokens_replace(pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma) %>%
tokens_wordstem() %>%
tokens_remove(c("sec","bill","act", "section", "funds", "shall","must", "use", "author")) %>%
dfm()
#Do we also want to lemmatize and apply word stems??
# tokens_replace(pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma) %>%
# tokens_wordstem() %>%
dfmat %>% colSums() %>% sort(decreasing = TRUE) %>% head(20)
dfmat_115 <- dfm_subset(dfmat, session == 115)
textplot_wordcloud(dfmat_115, max_words = 300)
dfmat_116 <- dfm_subset(dfmat, session == 116)
textplot_wordcloud(dfmat_116, max_words = 300)
dfmat_117 <- dfm_subset(dfmat, session == 117)
textplot_wordcloud(dfmat_117, max_words = 300)
dfmatRepDem <- dfm(corp, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups =
corp$party) %>%
dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmatRepDem, comparison = TRUE, max_words = 300,
color = c("green","blue", "red"))
corp_115 <- df %>% filter(session == 115) %>% rename(text = summary) %>% corpus()
modelpart15 <- dfm(corp_115, remove = stopwords("english"),remove_numbers = TRUE, remove_punct = TRUE, groups = corp_115$party) %>%
dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
mp15 <- textplot_wordcloud(modelpart15, comparison = TRUE, max_words = 300,
color = c("green","blue", "red"))
mp15
corp_116 <- df %>% filter(session == 116) %>% rename(text = summary) %>% corpus()
modelpart16 <- dfm(corp_116, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups = corp_116$party) %>% dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
mp16 <- textplot_wordcloud(modelpart16, comparison = TRUE, max_words = 300,
color = c("green","blue", "red"))
mp16
corp_117 <- df %>% filter(session == 117) %>% rename(text = summary) %>% corpus()
modelpart17 <- dfm(corp_117, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups = corp_117$party) %>% dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
mp17 <- textplot_wordcloud(modelpart17, comparison = TRUE, max_words = 300,
color = c("green","blue", "red"))
mp17
dfmatCon <- dfm(corp, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups = corp$session) %>%dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmatCon, comparison = TRUE, max_words = 300,
color = c("blue", "red"))
#Overall most common words
tfreq <- dfmat %>%
textstat_frequency() %>%
head(20)
tfreq$feature <- factor(tfreq$feature, levels=tfreq$feature)
ggplot(tfreq, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency",
x ="Frequency", y = "Feature") +
theme_minimal()
#Republicans
dfmat_r <- dfm_subset(dfmat, party == "Republican")
tfreq_r <- dfmat_r %>%
textstat_frequency() %>%
head(20)
tfreq_r$feature <- factor(tfreq_r$feature, levels=tfreq_r$feature)
ggplot(tfreq_r, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency for Republican Bills",
x ="Frequency", y = "Feature") +
theme_minimal()
#Democrats
dfmat_d <- dfm_subset(dfmat, party == "Democrat")
tfreq_d <- dfmat_d %>%
textstat_frequency() %>%
head(20)
tfreq_d$feature <- factor(tfreq_d$feature, levels=tfreq_d$feature)
ggplot(tfreq_d, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency for Democrat Bills",
x ="Frequency", y = "Feature") +
theme_minimal()
# Most common words by Congressional session 115
tfreq_115 <- dfmat_115 %>%
textstat_frequency() %>%
head(20)
tfreq_115$feature <- factor(tfreq_115$feature, levels=tfreq_115$feature)
ggplot(tfreq_115, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency for 115 session (2017 - 2018)",
x ="Frequency", y = "Feature") +
theme_minimal()
party_corp <- corpus_subset(corp,
party %in% c("Democrat", "Republican"))
# Create a dfm grouped by party
party_dfm <- tokens(party_corp, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%
tokens_remove(patter = stopwords("en")) %>%
tokens_replace(pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma) %>%
tokens_wordstem() %>%
tokens_remove(c("sec","bill","act", "section", "funds", "shall","must", "use", "author", "may")) %>%
tokens_group(groups = party) %>%
dfm()
# Calculate keyness and determine Republican as target group
result_keyness <- textstat_keyness(party_dfm, target = "Republican")
# Plot estimated word keyness
textplot_keyness(result_keyness)
# Most common words by Congressional session 115
tfreq_115 <- dfmat_115 %>%
textstat_frequency() %>%
head(20)
tfreq_115$feature <- factor(tfreq_115$feature, levels=tfreq_115$feature)
ggplot(tfreq_115, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency for 115 session (2017 - 2018)",
x ="Frequency", y = "Feature") +
theme_minimal()
# Most common words by Congressional session 116
tfreq_116 <- dfmat_116 %>%
textstat_frequency() %>%
head(20)
tfreq_116$feature <- factor(tfreq_116$feature, levels=tfreq_116$feature)
ggplot(tfreq_116, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency for 116 session (2019 - 2020)",
x ="Frequency", y = "Feature") +
theme_minimal()
# Most common words by Congressional session 117
tfreq_117 <- dfmat_117 %>%
textstat_frequency() %>%
head(20)
tfreq_117$feature <- factor(tfreq_117$feature, levels=tfreq_117$feature)
ggplot(tfreq_117, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency for 117 session (2021 - 2022)",
x ="Frequency", y = "Feature") +
theme_minimal()
df <- df %>%
mutate(session_written = ifelse(session == 115, "session_115",
ifelse(session == 116, "session_116",
ifelse(session == 117, "session_117", NA))))
ytfreq <- dfmat %>%
textstat_frequency(groups = df$session_written) %>%
pivot_wider(id_cols=feature, names_from=group, values_from=frequency)
#Comparing sessions 115 and 116
#Plot #1: differences in frequency by session
ggplot(ytfreq, aes(x= session_116, y= session_115)) +
geom_point() + theme_minimal()
#Plot #2: coloring changes by session
ytfreq$change_115_116 <- log(ytfreq$session_116 / ytfreq$session_115)
max_change <- max(abs(ytfreq$change_115_116), na.rm=TRUE)
ggplot(ytfreq, aes(x=session_116, y=session_115, fill=change_115_116)) +
geom_point(color="grey", shape=21) +
scale_fill_gradientn(
colors = c("#4575b4","white","#d73027"),
values = scales::rescale(c(max_change*-1,0,max_change)), limits = c(max_change*-1,max_change))+
theme_bw()
#Plot 3: labeling words with biggest change
labels <- ytfreq %>%
rowwise() %>%
mutate(max_value = max(session_116, session_115)) %>%
filter((abs(change_115_116)>0.4 & max_value>2.5) )
ggplot(ytfreq, aes(x=session_116, y=session_115, fill=change_115_116)) +
geom_point(color="grey", shape=21) +
scale_fill_gradientn(
colors = c("#4575b4","white","#d73027"),
values = scales::rescale(c(max_change*-1,0,max_change)), limits = c(max_change*-1,max_change)) + theme_bw() +
geom_label_repel(data=labels, aes(label=feature), min.segment.length = 0)
#Comparing sessions 116 and 117
ggplot(ytfreq, aes(x= session_117, y= session_116)) +
geom_point() + theme_minimal()
#Plot #2: coloring changes by session
ytfreq$change_116_117 <- log(ytfreq$session_117 / ytfreq$session_116)
max_change <- max(abs(ytfreq$change_116_117), na.rm=TRUE)
ggplot(ytfreq, aes(x=session_117, y=session_116, fill=change_116_117)) +
geom_point(color="grey", shape=21) +
scale_fill_gradientn(
colors = c("#4575b4","white","#d73027"),
values = scales::rescale(c(max_change*-1,0,max_change)), limits = c(max_change*-1,max_change))+
theme_bw()
#Plot 3: labeling words with biggest change
labels <- ytfreq %>%
rowwise() %>%
mutate(max_value = max(session_117, session_116)) %>%
filter((abs(change_116_117)>0.4 & max_value>2.5) )
ggplot(ytfreq, aes(x=session_117, y=session_116, fill=change_116_117)) +
geom_point(color="grey", shape=21) +
scale_fill_gradientn(
colors = c("#4575b4","white","#d73027"),
values = scales::rescale(c(max_change*-1,0,max_change)), limits = c(max_change*-1,max_change)) + theme_bw() +
geom_label_repel(data=labels, aes(label=feature), min.segment.length = 0)
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(readr)
library(dplyr)
library(quanteda.textplots)
library(uwot)
library(topicmodels)
library(tidytext)
library(ggrepel)
library(vader)
library(tidyr)
library(tidymodels)
library(textrecipes)
df <- read_csv("https://raw.githubusercontent.com/juka19/tad_assignment3/main/data/data_11_28.csv")
#if two thirds of the sponsors are democrats, we consider the bill democrat-dominated
#same for republicans
#if there is no clear majority, they are "Both"
df$party <- ifelse(df$cosponsor_D_perc > 0.66, "Democrat", ifelse(df$cosponsor_R_perc > 0.66, "Republican", "Both"))
ggplot(df, aes(x = cosponsor_D_perc)) +
geom_histogram(aes(y=..density..), colour="black", fill="white") +
geom_density(alpha=.1, fill="blue") +
labs(title="Density of bill cosposor party",
x ="Cosponsor party composition", y = "Density",
caption = "Numbers represent proportion of cosponsors from Democratic party,
so 0.0 represents bills that were fully Republican and 1.0 represents
bills that were fully Democrat.") +
theme_minimal()
df_corp <- df
df_corp <- df_corp %>% rename(text = summary)
corp <- corpus(df_corp)
dfmat <- corp %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%
tokens_remove(patter = stopwords("en")) %>%
tokens_replace(pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma) %>%
tokens_wordstem() %>%
tokens_remove(c("sec","bill","act", "section", "funds", "shall","must", "use", "author")) %>%
dfm()
#Do we also want to lemmatize and apply word stems??
# tokens_replace(pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma) %>%
# tokens_wordstem() %>%
dfmat %>% colSums() %>% sort(decreasing = TRUE) %>% head(20)
dfmat_115 <- dfm_subset(dfmat, session == 115)
textplot_wordcloud(dfmat_115, max_words = 300)
dfmat_116 <- dfm_subset(dfmat, session == 116)
textplot_wordcloud(dfmat_116, max_words = 300)
dfmat_117 <- dfm_subset(dfmat, session == 117)
textplot_wordcloud(dfmat_117, max_words = 300)
dfmatRepDem <- dfm(corp, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups =
corp$party) %>%
dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmatRepDem, comparison = TRUE, max_words = 300,
color = c("green","blue", "red"))
corp_115 <- df %>% filter(session == 115) %>% rename(text = summary) %>% corpus()
modelpart15 <- dfm(corp_115, remove = stopwords("english"),remove_numbers = TRUE, remove_punct = TRUE, groups = corp_115$party) %>%
dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
mp15 <- textplot_wordcloud(modelpart15, comparison = TRUE, max_words = 300,
color = c("green","blue", "red"))
mp15
corp_116 <- df %>% filter(session == 116) %>% rename(text = summary) %>% corpus()
modelpart16 <- dfm(corp_116, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups = corp_116$party) %>% dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
mp16 <- textplot_wordcloud(modelpart16, comparison = TRUE, max_words = 300,
color = c("green","blue", "red"))
mp16
corp_117 <- df %>% filter(session == 117) %>% rename(text = summary) %>% corpus()
modelpart17 <- dfm(corp_117, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups = corp_117$party) %>% dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
mp17 <- textplot_wordcloud(modelpart17, comparison = TRUE, max_words = 300,
color = c("green","blue", "red"))
mp17
dfmatCon <- dfm(corp, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups = corp$session) %>%dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmatCon, comparison = TRUE, max_words = 300,
color = c("blue", "red"))
#Overall most common words
tfreq <- dfmat %>%
textstat_frequency() %>%
head(20)
tfreq$feature <- factor(tfreq$feature, levels=tfreq$feature)
ggplot(tfreq, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency",
x ="Frequency", y = "Feature") +
theme_minimal()
setwd('GitHub\\tad_assignment3')
setwd('GitHub\\tad_assignment3')
use_python("tad_venv3\\Scripts\\python.exe")
library(reticulate)
use_python("tad_venv3\\Scripts\\python.exe")
library(htmltools)
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(readr)
library(dplyr)
library(quanteda.textplots)
library(uwot)
library(topicmodels)
library(tidytext)
library(ggrepel)
library(vader)
library(tidyr)
library(tidymodels)
library(textrecipes)
library(plotly)
library(reticulate)
library(igraph)
library(rjson)
library(visNetwork)
knitr::opts_chunk$set(echo = TRUE)
library(htmltools)
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(readr)
library(dplyr)
library(quanteda.textplots)
library(uwot)
library(topicmodels)
library(tidytext)
library(ggrepel)
library(vader)
library(tidyr)
library(tidymodels)
library(textrecipes)
library(plotly)
library(reticulate)
library(igraph)
library(rjson)
library(visNetwork)
summary_sentiment <- read_csv("https://raw.githubusercontent.com/juka19/tad_assignment3/main/data/data_w_vader.csv")
summary_sentiment$party <- ifelse(summary_sentiment$cosponsor_D_perc > 0.66, "Democrat", ifelse(summary_sentiment$cosponsor_R_perc > 0.66, "Republican", "Both"))
wide_sentiment <- summary_sentiment %>%
group_by(party, date) %>%
summarise(score = mean(compound)) %>%
pivot_wider(names_from = party, values_from = score) %>%
select(-c("Both", "NA"))
days <- data.frame(date = seq(as.Date("2017-01-01"),as.Date("2022-12-31"),1))
daily_sentiment <- days %>%
left_join(wide_sentiment) %>%
pivot_longer(cols = -date, names_to="party", values_to="score")
p4 <- ggplot(daily_sentiment, aes(x=date, y = score, colour=party)) +
geom_point(aes(y=score), size=1) +
theme_minimal() +
geom_smooth(method = "loess", se = FALSE)+
scale_color_manual(values = c("blue","red"))
ggplotly(p4)
df <- read_csv("https://raw.githubusercontent.com/juka19/tad_assignment3/main/data/data_11_28.csv")
corp2 <- corpus(df$summary)
dfmat2 <- corp2 %>%
tokens(remove_punct = TRUE) %>%
tokens_remove(patter = stopwords("en")) %>%
dfm() %>%
dfm_trim(min_termfreq = 5)
embeddings <- umap(as.matrix(dfmat2))
df$x <- embeddings[,1]
df$y <- embeddings[,2]
colordict <- c( "Democrat"="blue","Republican"="red", "Both"="yellow")
p <- ggplot(df, aes(x, y, fill=party)) +
geom_point(color="grey", shape=21, size=0.5) +
scale_fill_manual(values=colordict) +
theme_bw()
p <- ggplotly(p)
df$party <- ifelse(df$cosponsor_D_perc > 0.66, "Democrat", ifelse(df$cosponsor_R_perc > 0.66, "Republican", "Both"))
p <- ggplot(df, aes(x, y, fill=party)) +
geom_point(color="grey", shape=21, size=0.5) +
scale_fill_manual(values=colordict) +
theme_bw()
p <- ggplotly(p)
p
p <- ggplot(df, aes(x, y, fill=party)) +
geom_point(color="grey", shape=21, size=0.5) +
labs(title = "Dimensionality of all summary texts", fill = "Bill sponsorship")
p <- ggplot(df, aes(x, y, fill=party)) +
geom_point(color="grey", shape=21, size=0.5) +
labs(title = "Dimensionality of all summary texts", fill = "Bill sponsorship") +
scale_fill_manual(values=colordict) +
theme_bw()
p <- ggplotly(p)
p
p <- ggplot(df, aes(x, y, fill=party)) +
geom_point(color="grey", shape=21, size=0.5) +
labs(title = "Dimensionality of all summary texts", x = "X", y = "Y",
fill = "Bill sponsorship") +
scale_fill_manual(values=colordict) +
theme_bw()
p <- ggplotly(p)
p
df1 <- df %>%
mutate(party_full = ifelse(cosponsor_D_perc == 1.0, "Dem",
ifelse(cosponsor_R_perc == 1.0, "Rep", NA))) %>%
drop_na(party_full)
corp3 <- corpus(df1$summary)
dfmat3 <- corp3 %>%
tokens(remove_punct = TRUE) %>%
tokens_remove(patter = stopwords("en")) %>%
dfm() %>%
dfm_trim(min_termfreq = 5)
embeddings2 <- umap(as.matrix(dfmat3))
df1$x <- embeddings2[,1]
df1$y <- embeddings2[,2]
colordict2 <- c( "Democrat"="blue","Republican"="red")
j <- ggplot(df1, aes(x, y, fill=party)) +
geom_point(color="grey", shape=21, size=0.5) +
scale_fill_manual(values=colordict2) +
theme_bw()
j <- ggplotly(j)
j
p <- ggplot(df, aes(x, y, fill=party)) +
geom_point(color="grey", shape=21, size=0.5) +
labs(title = "Dimensionality of all summary texts", x = "X", y = "Y",
fill = "Bill sponsorship",
caption = "All bills represented, Democrat are bills with > 0.66 Democrat,
Republican are bills with > 0.66 Republican, Both are bills in between.") +
scale_fill_manual(values=colordict) +
theme_bw()
p <- ggplotly(p)
p
p <- ggplot(df, aes(x, y, fill=party)) +
geom_point(color="grey", shape=21, size=0.5) +
labs(title = "Dimensionality of all summary texts", x = "X", y = "Y",
fill = "Bill sponsorship",
caption = "All bills represented, Democrat are bills with > 0.66 Democrat,
Republican are bills with > 0.66 Republican, Both are bills in between.") +
scale_fill_manual(values=colordict) +
theme_bw()
p <- ggplotly(p)
p
df1 <- df %>%
mutate(party_full = ifelse(cosponsor_D_perc == 1.0, "Dem",
ifelse(cosponsor_R_perc == 1.0, "Rep", NA))) %>%
drop_na(party_full)
View(df1)
j <- ggplot(df1, aes(x, y, fill=party)) +
geom_point(color="grey", shape=21, size=0.5) +
labs(title = "Dimensionality of partisan summary texts", x = "X", y = "Y",
fill = "Bill sponsorship",
caption = "Bills with only completely Democrat of Republican sponsorship
represented.") +
scale_fill_manual(values=colordict2) +
theme_bw()
j <- ggplotly(j)
j
ggplot(df1, aes(x, y, fill=party)) +
geom_point(color="grey", shape=21, size=0.5) +
labs(title = "Dimensionality of partisan summary texts", x = "X", y = "Y",
fill = "Bill sponsorship",
caption = "Bills with only completely Democrat of Republican sponsorship
represented.") +
scale_fill_manual(values=colordict2) +
theme_bw()
