fromJSON() %>%
pluck(4) %>%
as.data.frame()
GET(paste0(baseurl, query)) %>%
content(as = 'text') %>%
fromJSON() %>% str()
GET(paste0(baseurl, query)) %>%
content(results, as = 'text') %>%
fromJSON()
GET(paste0(baseurl, query)) %>%
content('results', as = 'text') %>%
fromJSON()
GET(paste0(baseurl, query)) %>%
content(as = 'text') %>%
fromJSON() %>%
pluck(4) %>%
as.data.frame()
baseurl <- 'https://api.congress.gov/v3/'
query <- 'nomination/115/2259/actions'
GET(paste0(baseurl, query),
authenticate(user = Sys.getenv('US.GOV_API'), password = '')) %>%
content(as = 'text') %>%
fromJSON() %>% str()
baseurl <- 'https://api.congress.gov/v3/'
query <- 'nomination/115/2259/actions'
GET(paste0(baseurl, query),
authenticate(user = Sys.getenv('US.GOV_API'), password = '')) %>%
content(as = 'text') %>%
fromJSON() %>%
pluck(1) %>%
as.data.frame()
query <- paste0('summaries/117/hr?fromDateTime=2022-10-01T00:00:00Z&toDateTime=2022-11-01T00:00:00Z&sort=updateDate+desc')
GET(paste0(baseurl, query), query = list(api_key = Sys.getenv('US.GOV_API'))) %>%
content(as = 'text') %>%
fromJSON() %>%
pluck(1) %>%
as.data.frame()
GET(paste0(baseurl, query), query = list(api_key = Sys.getenv('US.GOV_API'))) %>%
content(as = 'text') %>%
fromJSON() %>% str()
query <- paste0('summaries/117/hr?fromDateTime=2022-10-01T00:00:00Z&toDateTime=2022-11-01T00:00:00Z&sort=updateDate+desc')
GET(paste0(baseurl, query), query = list(api_key = Sys.getenv('US.GOV_API'))) %>%
content(as = 'text') %>%
fromJSON() %>%
pluck(3) %>%
as.data.frame()
emo::ji("book")
library(bupaR)
install.packages("bupaR", dependencies = T)
install.packages("bupaR", dependencies = T)
install.packages("bupaR", dependencies = T)
install.packages("bupaR", dependencies = T)
detach("package:stats", unload = TRUE)
detach("package:utils", unload = TRUE)
detach("package:methods", unload = TRUE)
detach("package:grDevices", unload = TRUE)
detach("package:datasets", unload = TRUE)
detach("package:graphics", unload = TRUE)
install.packages("bupaR", dependencies = T)
library(utils, lib.loc = "C:/Program Files/R/R-4.1.1/library")
install.packages("bupaR", dependencies = T)
spons_data %>%
select(-c(X, ...1, ...2, Unnamed..0)) %>%
write.csv('GitHub\\tad_assignment3\\data\\data_28_11_sponsors.csv', row.names = F)
library(jsonlite)
library(bupaR)
library(eventdataR)
library(processanimateR)
library(tidyverse)
json <- fromJSON("GitHub\\tad_assignment3\\data\\json_output.txt")
data <- read.csv("GitHub\\tad_assignment3\\data\\data_11_28.csv")
library(rvest)
library(curl)
json <- fromJSON("GitHub\\tad_assignment3\\data\\json_output.txt")
json <- fromJSON(file = "GitHub\\tad_assignment3\\data\\json_output.txt")
?fromJSON
json <- fromJSON(txt = "GitHub\\tad_assignment3\\data\\json_output.txt")
json <- fromJSON(file = "GitHub\\tad_assignment3\\data\\json_output.txt")
library(rjson)
json <- fromJSON(file = "GitHub\\tad_assignment3\\data\\json_output.txt")
getwd()
setwd('../')
json <- fromJSON(file = "GitHub\\tad_assignment3\\data\\json_output.txt")
json <- fromJSON(file = "GitHub\\tad_assignment3\\no.txt")
list.dirs()
json <- fromJSON(file = "GitHub\\tad_assignment3\\no.txt")
setwd('Documents')
json <- fromJSON(file = "GitHub\\tad_assignment3\\no.txt")
data <- read.csv("GitHub\\tad_assignment3\\data\\data_11_28.csv")
pmap_dfr(
list(
json$metadata$bill$sponsors,
json$`bill number`,
json$actions
),
\(x, y, z) z %>%
rbind() %>%
cbind(y) %>%
cbind(x['party'])
) -> js_df
map_dfr(
unique(json$`bill number`),
function(y) {
a <- json %>% filter(`bill number` == y)
x <- a %>% pluck('metadata', 'bill', 'sponsors')
tibble('bill.number' = y,
'sponsor1_name' = x %>% pluck(1, 'fullName'),
'sponsor2_name' = x %>% pluck(2, 'fullName'),
'sponsor3_name' = x %>% pluck(3, 'fullName'),
'sponsor1_party' = x %>% pluck(1, 'party'),
'sponsor2_party' = x %>% pluck(2, 'party'),
'sponsor3_party' = x %>% pluck(3, 'party'),
'sponsor1_id' = x %>% pluck(1, 'bioguidedId'),
'sponsor2_id' = x %>% pluck(2, 'bioguidedId'),
'sponsor3_id' = x %>% pluck(3, 'bioguidedId'))
}
) -> spons_df
spons_data <- data %>%
left_join(spons_df %>%
mutate(bill.number = as.integer(bill.number)))
json %>% as.tibble() -> test
json %>% str()
View(spons_df)
js <- map_dfr(js, cbind)
js <- map_dfr(json, cbind)
json %>% names()
json[1]
json %>% as.data.frame()
json <- as.data.frame(json)
View(json)
json <- fromJSON(file = "GitHub\\tad_assignment3\\json.txt")
add_info <- read_csv("GitHub\\tad_assignment3\\data\\additional_info_final.csv")
add_info %>% colnames()
View(add_info)
add_info$metadata %>%
map_dfr(fromJSON) -> test
add_info$metadata %>% toJSON() %>% fromJSON ->test
test
json_1 <- json %>% toJSON() %>% fromJSON()
length(json_1)
add_info$metadata %>%
map_dfr(function(x) x %>%
toJSON() %>%
fromJSON()) -> test
add_info$metadata %>%
map_dfr(function(x) x %>%
toJSON() %>%
fromJSON() %>%
as.data.frame()) -> test
test[1] %>% fromJSON()
test$.[1] %>% fromJSON()
json <- fromJSON(file = "GitHub\\tad_assignment3\\no.txt")
gc()
library(bupaR)
library(eventdataR)
library(processanimateR)
library(tidyverse)
library(rjson)
json <- fromJSON(file = "GitHub\\tad_assignment3\\no.txt")
test <- json %>% as.tibble()
as.data.frame(json) -> test
json <- fromJSON(file = "GitHub\\tad_assignment3\\no.txt", simplify = T)
json %>% names(+)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
install.packages('tidyverse')
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
install.packages('tidyverse')
library(tidyverse)
.libPaths()
install.packages('rlang')
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidyverse, lib.loc = .libPaths()[1])
library(tidyverse, lib.loc = .libPaths()[2])
remove.packages('rlang')
remove.packages('rlang', lib = .libPaths()[1])
remove.packages('rlang', lib = .libPaths()[2])
remove.packages('rlang', lib = .libPaths()[3])
.libPaths()
.libPaths()[2]
.libPaths()[1]
install.packages('rlang')
install.packages("rlang")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(readr)
library(dplyr)
library(quanteda.textplots)
library(uwot)
library(topicmodels)
library(tidytext)
library(ggrepel)
library(vader)
library(tidyr)
library(tidymodels)
library(textrecipes)
df <- read_csv("https://raw.githubusercontent.com/juka19/tad_assignment3/main/data/data_11_28.csv")
#if two thirds of the sponsors are democrats, we consider the bill democrat-dominated
#same for republicans
#if there is no clear majority, they are "Both"
df$party <- ifelse(df$cosponsor_D_perc > 0.66, "Democrat", ifelse(df$cosponsor_R_perc > 0.66, "Republican", "Both"))
ggplot(df, aes(x = cosponsor_D_perc)) +
geom_histogram(aes(y=..density..), colour="black", fill="white") +
geom_density(alpha=.1, fill="blue") +
labs(title="Density of bill cosposor party",
x ="Cosponsor party composition", y = "Density",
caption = "Numbers represent proportion of cosponsors from Democratic party,
so 0.0 represents bills that were fully Republican and 1.0 represents
bills that were fully Democrat.") +
theme_minimal()
df_corp <- df
df_corp <- df_corp %>% rename(text = summary)
corp <- corpus(df_corp)
dfmat <- corp %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%
tokens_remove(patter = stopwords("en")) %>%
tokens_replace(pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma) %>%
tokens_wordstem() %>%
tokens_remove(c("sec","bill","act", "section", "funds", "shall","must", "use", "author")) %>%
dfm()
#Do we also want to lemmatize and apply word stems??
# tokens_replace(pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma) %>%
# tokens_wordstem() %>%
dfmat %>% colSums() %>% sort(decreasing = TRUE) %>% head(20)
dfmat_115 <- dfm_subset(dfmat, session == 115)
textplot_wordcloud(dfmat_115, max_words = 300)
dfmat_116 <- dfm_subset(dfmat, session == 116)
textplot_wordcloud(dfmat_116, max_words = 300)
dfmat_117 <- dfm_subset(dfmat, session == 117)
textplot_wordcloud(dfmat_117, max_words = 300)
dfmatRepDem <- dfm(corp, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups =
corp$party) %>%
dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmatRepDem, comparison = TRUE, max_words = 300,
color = c("green","blue", "red"))
corp_115 <- df %>% filter(session == 115) %>% rename(text = summary) %>% corpus()
modelpart15 <- dfm(corp_115, remove = stopwords("english"),remove_numbers = TRUE, remove_punct = TRUE, groups = corp_115$party) %>%
dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
mp15 <- textplot_wordcloud(modelpart15, comparison = TRUE, max_words = 300,
color = c("green","blue", "red"))
mp15
corp_116 <- df %>% filter(session == 116) %>% rename(text = summary) %>% corpus()
modelpart16 <- dfm(corp_116, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups = corp_116$party) %>% dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
mp16 <- textplot_wordcloud(modelpart16, comparison = TRUE, max_words = 300,
color = c("green","blue", "red"))
mp16
corp_117 <- df %>% filter(session == 117) %>% rename(text = summary) %>% corpus()
modelpart17 <- dfm(corp_117, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups = corp_117$party) %>% dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
mp17 <- textplot_wordcloud(modelpart17, comparison = TRUE, max_words = 300,
color = c("green","blue", "red"))
mp17
dfmatCon <- dfm(corp, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups = corp$session) %>%dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmatCon, comparison = TRUE, max_words = 300,
color = c("blue", "red"))
#Overall most common words
tfreq <- dfmat %>%
textstat_frequency() %>%
head(20)
tfreq$feature <- factor(tfreq$feature, levels=tfreq$feature)
ggplot(tfreq, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency",
x ="Frequency", y = "Feature") +
theme_minimal()
#Republicans
dfmat_r <- dfm_subset(dfmat, party == "Republican")
tfreq_r <- dfmat_r %>%
textstat_frequency() %>%
head(20)
tfreq_r$feature <- factor(tfreq_r$feature, levels=tfreq_r$feature)
ggplot(tfreq_r, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency for Republican Bills",
x ="Frequency", y = "Feature") +
theme_minimal()
#Democrats
dfmat_d <- dfm_subset(dfmat, party == "Democrat")
tfreq_d <- dfmat_d %>%
textstat_frequency() %>%
head(20)
tfreq_d$feature <- factor(tfreq_d$feature, levels=tfreq_d$feature)
ggplot(tfreq_d, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency for Democrat Bills",
x ="Frequency", y = "Feature") +
theme_minimal()
# Most common words by Congressional session 115
tfreq_115 <- dfmat_115 %>%
textstat_frequency() %>%
head(20)
tfreq_115$feature <- factor(tfreq_115$feature, levels=tfreq_115$feature)
ggplot(tfreq_115, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency for 115 session (2017 - 2018)",
x ="Frequency", y = "Feature") +
theme_minimal()
party_corp <- corpus_subset(corp,
party %in% c("Democrat", "Republican"))
# Create a dfm grouped by party
party_dfm <- tokens(party_corp, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%
tokens_remove(patter = stopwords("en")) %>%
tokens_replace(pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma) %>%
tokens_wordstem() %>%
tokens_remove(c("sec","bill","act", "section", "funds", "shall","must", "use", "author", "may")) %>%
tokens_group(groups = party) %>%
dfm()
# Calculate keyness and determine Republican as target group
result_keyness <- textstat_keyness(party_dfm, target = "Republican")
# Plot estimated word keyness
textplot_keyness(result_keyness)
# Most common words by Congressional session 115
tfreq_115 <- dfmat_115 %>%
textstat_frequency() %>%
head(20)
tfreq_115$feature <- factor(tfreq_115$feature, levels=tfreq_115$feature)
ggplot(tfreq_115, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency for 115 session (2017 - 2018)",
x ="Frequency", y = "Feature") +
theme_minimal()
# Most common words by Congressional session 116
tfreq_116 <- dfmat_116 %>%
textstat_frequency() %>%
head(20)
tfreq_116$feature <- factor(tfreq_116$feature, levels=tfreq_116$feature)
ggplot(tfreq_116, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency for 116 session (2019 - 2020)",
x ="Frequency", y = "Feature") +
theme_minimal()
# Most common words by Congressional session 117
tfreq_117 <- dfmat_117 %>%
textstat_frequency() %>%
head(20)
tfreq_117$feature <- factor(tfreq_117$feature, levels=tfreq_117$feature)
ggplot(tfreq_117, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency for 117 session (2021 - 2022)",
x ="Frequency", y = "Feature") +
theme_minimal()
df <- df %>%
mutate(session_written = ifelse(session == 115, "session_115",
ifelse(session == 116, "session_116",
ifelse(session == 117, "session_117", NA))))
ytfreq <- dfmat %>%
textstat_frequency(groups = df$session_written) %>%
pivot_wider(id_cols=feature, names_from=group, values_from=frequency)
#Comparing sessions 115 and 116
#Plot #1: differences in frequency by session
ggplot(ytfreq, aes(x= session_116, y= session_115)) +
geom_point() + theme_minimal()
#Plot #2: coloring changes by session
ytfreq$change_115_116 <- log(ytfreq$session_116 / ytfreq$session_115)
max_change <- max(abs(ytfreq$change_115_116), na.rm=TRUE)
ggplot(ytfreq, aes(x=session_116, y=session_115, fill=change_115_116)) +
geom_point(color="grey", shape=21) +
scale_fill_gradientn(
colors = c("#4575b4","white","#d73027"),
values = scales::rescale(c(max_change*-1,0,max_change)), limits = c(max_change*-1,max_change))+
theme_bw()
#Plot 3: labeling words with biggest change
labels <- ytfreq %>%
rowwise() %>%
mutate(max_value = max(session_116, session_115)) %>%
filter((abs(change_115_116)>0.4 & max_value>2.5) )
ggplot(ytfreq, aes(x=session_116, y=session_115, fill=change_115_116)) +
geom_point(color="grey", shape=21) +
scale_fill_gradientn(
colors = c("#4575b4","white","#d73027"),
values = scales::rescale(c(max_change*-1,0,max_change)), limits = c(max_change*-1,max_change)) + theme_bw() +
geom_label_repel(data=labels, aes(label=feature), min.segment.length = 0)
#Comparing sessions 116 and 117
ggplot(ytfreq, aes(x= session_117, y= session_116)) +
geom_point() + theme_minimal()
#Plot #2: coloring changes by session
ytfreq$change_116_117 <- log(ytfreq$session_117 / ytfreq$session_116)
max_change <- max(abs(ytfreq$change_116_117), na.rm=TRUE)
ggplot(ytfreq, aes(x=session_117, y=session_116, fill=change_116_117)) +
geom_point(color="grey", shape=21) +
scale_fill_gradientn(
colors = c("#4575b4","white","#d73027"),
values = scales::rescale(c(max_change*-1,0,max_change)), limits = c(max_change*-1,max_change))+
theme_bw()
#Plot 3: labeling words with biggest change
labels <- ytfreq %>%
rowwise() %>%
mutate(max_value = max(session_117, session_116)) %>%
filter((abs(change_116_117)>0.4 & max_value>2.5) )
ggplot(ytfreq, aes(x=session_117, y=session_116, fill=change_116_117)) +
geom_point(color="grey", shape=21) +
scale_fill_gradientn(
colors = c("#4575b4","white","#d73027"),
values = scales::rescale(c(max_change*-1,0,max_change)), limits = c(max_change*-1,max_change)) + theme_bw() +
geom_label_repel(data=labels, aes(label=feature), min.segment.length = 0)
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(readr)
library(dplyr)
library(quanteda.textplots)
library(uwot)
library(topicmodels)
library(tidytext)
library(ggrepel)
library(vader)
library(tidyr)
library(tidymodels)
library(textrecipes)
df <- read_csv("https://raw.githubusercontent.com/juka19/tad_assignment3/main/data/data_11_28.csv")
#if two thirds of the sponsors are democrats, we consider the bill democrat-dominated
#same for republicans
#if there is no clear majority, they are "Both"
df$party <- ifelse(df$cosponsor_D_perc > 0.66, "Democrat", ifelse(df$cosponsor_R_perc > 0.66, "Republican", "Both"))
ggplot(df, aes(x = cosponsor_D_perc)) +
geom_histogram(aes(y=..density..), colour="black", fill="white") +
geom_density(alpha=.1, fill="blue") +
labs(title="Density of bill cosposor party",
x ="Cosponsor party composition", y = "Density",
caption = "Numbers represent proportion of cosponsors from Democratic party,
so 0.0 represents bills that were fully Republican and 1.0 represents
bills that were fully Democrat.") +
theme_minimal()
df_corp <- df
df_corp <- df_corp %>% rename(text = summary)
corp <- corpus(df_corp)
dfmat <- corp %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%
tokens_remove(patter = stopwords("en")) %>%
tokens_replace(pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma) %>%
tokens_wordstem() %>%
tokens_remove(c("sec","bill","act", "section", "funds", "shall","must", "use", "author")) %>%
dfm()
#Do we also want to lemmatize and apply word stems??
# tokens_replace(pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma) %>%
# tokens_wordstem() %>%
dfmat %>% colSums() %>% sort(decreasing = TRUE) %>% head(20)
dfmat_115 <- dfm_subset(dfmat, session == 115)
textplot_wordcloud(dfmat_115, max_words = 300)
dfmat_116 <- dfm_subset(dfmat, session == 116)
textplot_wordcloud(dfmat_116, max_words = 300)
dfmat_117 <- dfm_subset(dfmat, session == 117)
textplot_wordcloud(dfmat_117, max_words = 300)
dfmatRepDem <- dfm(corp, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups =
corp$party) %>%
dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmatRepDem, comparison = TRUE, max_words = 300,
color = c("green","blue", "red"))
corp_115 <- df %>% filter(session == 115) %>% rename(text = summary) %>% corpus()
modelpart15 <- dfm(corp_115, remove = stopwords("english"),remove_numbers = TRUE, remove_punct = TRUE, groups = corp_115$party) %>%
dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
mp15 <- textplot_wordcloud(modelpart15, comparison = TRUE, max_words = 300,
color = c("green","blue", "red"))
mp15
corp_116 <- df %>% filter(session == 116) %>% rename(text = summary) %>% corpus()
modelpart16 <- dfm(corp_116, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups = corp_116$party) %>% dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
mp16 <- textplot_wordcloud(modelpart16, comparison = TRUE, max_words = 300,
color = c("green","blue", "red"))
mp16
corp_117 <- df %>% filter(session == 117) %>% rename(text = summary) %>% corpus()
modelpart17 <- dfm(corp_117, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups = corp_117$party) %>% dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
mp17 <- textplot_wordcloud(modelpart17, comparison = TRUE, max_words = 300,
color = c("green","blue", "red"))
mp17
dfmatCon <- dfm(corp, remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, groups = corp$session) %>%dfm_remove(c("sec","bill","act", "section", "funds", "shall","must", "used")) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmatCon, comparison = TRUE, max_words = 300,
color = c("blue", "red"))
#Overall most common words
tfreq <- dfmat %>%
textstat_frequency() %>%
head(20)
tfreq$feature <- factor(tfreq$feature, levels=tfreq$feature)
ggplot(tfreq, aes(x=frequency, y=feature)) +
geom_col() + labs(title="Feature (word) frequency",
x ="Frequency", y = "Feature") +
theme_minimal()
setwd('GitHub\\tad_assignment3')
setwd('GitHub\\tad_assignment3')
use_python("tad_venv3\\Scripts\\python.exe")
library(reticulate)
use_python("tad_venv3\\Scripts\\python.exe")
library(htmltools)
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(readr)
library(dplyr)
library(quanteda.textplots)
library(uwot)
library(topicmodels)
library(tidytext)
library(ggrepel)
library(vader)
library(tidyr)
library(tidymodels)
library(textrecipes)
library(plotly)
library(reticulate)
library(igraph)
library(rjson)
library(visNetwork)
